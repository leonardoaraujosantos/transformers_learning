{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Layers\n",
    "Investigate and play with our transformer implementation\n",
    "\n",
    "#### References\n",
    "* https://www.tensorflow.org/text/tutorials/transformer\n",
    "* https://nlp.seas.harvard.edu/2018/04/03/attention.html#full-model\n",
    "* https://theaisummer.com/self-attention/\n",
    "* https://arxiv.org/pdf/2009.06732.pdf\n",
    "* https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "* https://bgg.medium.com/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from model.sublayers.transformer import scaled_dot_product_attn, MultiHeadedAttention\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled-Dot-Product\n",
    "<img src=\"./imgs/Scaled_Dot-Product_Attention.png\" width=\"320\" height=\"240\" align=\"right\">\n",
    "This operator enable the information retrieval system behaviour typical on transformers, where some query tensor will be used to search some concept encoded on the values, the key tensor is the index that allows queries to find the values.\n",
    "\n",
    "$$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "It's valid to mention that the matrix multiplication between Q and K is material for a lot of other paper that try to make the memory and compute consumption non-quadratic to the sequence lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled dot product attention shape: torch.Size([1, 2])\n",
      "Attention Weights shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "keys = torch.rand(4, 3)\n",
    "query = torch.rand(1, 3)\n",
    "values = torch.rand(4, 2)\n",
    "attn, attn_weights = scaled_dot_product_attn(query, keys, values)\n",
    "print('Scaled dot product attention shape:', attn.shape)\n",
    "print('Attention Weights shape:', attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention\n",
    "<img src=\"./imgs/MultiHeadAttn.png\" width=\"320\" height=\"240\" align=\"right\">\n",
    "This is the module that allow the transformer to learn(which means has parameters) how each word(token) in a input prase (or any sequence) relate to each other at different positions. The multiple \"heads\" also gives the possibility to represent those relations in different subspaces. \n",
    "\n",
    "$$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadedAttention(\n",
      "  (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (linear_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      ")\n",
      "Multiheaded attention output shape: torch.Size([1, 60, 512])\n",
      "Multiheaded attention weights: torch.Size([1, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "mha = MultiHeadedAttention(num_heads=8, d_model=512)\n",
    "print(mha)\n",
    "# [batch_size, encoder_sequence, d_model]\n",
    "x = torch.rand(1, 60, 512)\n",
    "attn, attn_weights = mha(key=x, query=x, value=x)\n",
    "\n",
    "# [batch_size, encoder_sequence, d_model]\n",
    "print(f'Multiheaded attention output shape: {attn.shape}')\n",
    "\n",
    "# [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "print(f'Multiheaded attention weights: {attn_weights.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
