{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Layers\n",
    "Investigate and play with our transformer implementation, this notebook is used to investigate how the information flows on each of the transformer component, if you want to undertand more the implementation refer to the source code on the model directory.\n",
    "<img src=\"./imgs/transformer_enc_dec.png\" width=\"320\" height=\"240\" align=\"left\">\n",
    "\n",
    "#### References\n",
    "* https://www.tensorflow.org/text/tutorials/transformer\n",
    "* https://nlp.seas.harvard.edu/2018/04/03/attention.html#full-model\n",
    "* https://theaisummer.com/self-attention/\n",
    "* https://arxiv.org/pdf/2009.06732.pdf\n",
    "* https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "* https://bgg.medium.com/seq2seq-pay-attention-to-self-attention-part-2-cf81bf32c73d\n",
    "* https://towardsdatascience.com/illustrated-guide-to-transformers-step-by-step-explanation-f74876522bc0\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
    "* https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html\n",
    "* https://discuss.pytorch.org/t/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential/5463\n",
    "* https://stats.stackexchange.com/questions/474440/why-do-transformers-use-layer-norm-instead-of-batch-norm\n",
    "* https://www.tensorflow.org/api_docs/python/tf/linalg/band_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from model.sublayers.transformer import WordEmbeddings, PositionalEncoding, TransformerEncoderDecoder\n",
    "from model.sublayers.transformer import scaled_dot_product_attn, MultiHeadedAttention, PositionWiseFeedForward\n",
    "from model.sublayers.transformer import EncoderBlock, DecoderBlock, TransformerDecoder, TransformerEncoder\n",
    "from model.sublayers.transformer import create_padding_mask, create_look_ahead_mask\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "seaborn.set_context(context=\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input/Output Embedding\n",
    "<img src=\"./imgs/embeddings.png\" width=\"640\" height=\"240\" align=\"up\">\n",
    "The first step of the architecture is to convert a sequence of tokens (words, characters) with shape [batch, seq_len] to a vector dimension $d_{\\text{model}}$ of shape [batch, seq_len, d_model], where $d_{model}$ will be the hidden dimension used throughout the whole architecture.\n",
    "\n",
    "For projects involving NLP we might well use nn.Embedding or Word2Vec but other projects involving Audio, Video, or Images use other embeddings mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentence = torch.randint(0, 200, (1, 3))\n",
    "word_emb = WordEmbeddings(d_model=4, vocab_size=200)\n",
    "vector = word_emb(input_sentence)\n",
    "print('Input shape:', input_sentence.shape)\n",
    "print('Input data:\\n', input_sentence)\n",
    "print('Vector shape:', vector.shape)\n",
    "print('vector data:\\n', vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "<img src=\"./imgs/pos_encoding.png\" width=\"640\" height=\"240\" align=\"up\">\n",
    "The transformer blocks while learning how tokens correlate to each other, it completely ignore the order of the tokens, to solve this problem the positional encoding increment into the tensors some scalar values that encode the order where the tokens appeared.\n",
    "\n",
    "By using sin/cos functions with different frequencies we take advatnage of the different phases w.r.t to the position $pos$\n",
    "\n",
    "$$ PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\\\ PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 50\n",
    "d_model=20\n",
    "pe = PositionalEncoding(d_model=d_model, dropout=0, max_len=max_seq_len)\n",
    "\n",
    "# Input embeddign without order information [batch, seq_len, d_model]\n",
    "input_embedding = pe.forward(torch.zeros(1, max_seq_len, d_model))\n",
    "print('Input Embedding shape:', input_embedding.shape)\n",
    "\n",
    "# Prepare plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "# Select first batch, all the sequence, dimensions 4 to 7\n",
    "plt.plot(np.arange(max_seq_len), input_embedding[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\"%p for p in [4,5,6,7]]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled-Dot-Product\n",
    "<img src=\"./imgs/Scaled_Dot-Product_Attention.png\" width=\"320\" height=\"240\" align=\"right\">\n",
    "This operator enable the information retrieval system behaviour typical on transformers, where some query tensor will be used to search some concept encoded on the values, the key tensor is the index that allows queries to find the values.\n",
    "\n",
    "$$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "It's valid to mention that the matrix multiplication between Q and K is material for a lot of other paper that try to make the memory and compute consumption non-quadratic to the sequence lenght."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = torch.rand(4, 3)\n",
    "query = torch.rand(1, 3)\n",
    "values = torch.rand(4, 2)\n",
    "attn, attn_weights = scaled_dot_product_attn(query, keys, values)\n",
    "print('Scaled dot product attention shape:', attn.shape)\n",
    "print('Attention Weights shape:', attn_weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention\n",
    "<img src=\"./imgs/MultiHeadAttn.png\" width=\"320\" height=\"240\" align=\"right\">\n",
    "This is the module that allow the transformer to learn(which means has parameters) how each word(token) in a input prase (or any sequence) relate to each other at different positions. The multiple \"heads\" also gives the possibility to represent those relations in different subspaces. \n",
    "\n",
    "$$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
    "\\mathrm{head_h})W^O    \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadedAttention(num_heads=8, d_model=512)\n",
    "print(mha)\n",
    "# [batch_size, encoder_seq_len, d_model]\n",
    "x = torch.rand(3, 60, 512)\n",
    "attn, attn_weights = mha(key=x, query=x, value=x, mask=None)\n",
    "\n",
    "# [batch_size, encoder_sequence, d_model]\n",
    "print(f'Multiheaded attention output shape: {attn.shape}')\n",
    "\n",
    "# [batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "print(f'Multiheaded attention weights: {attn_weights.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Point-wise Feed Forward\n",
    "<img src=\"./imgs/pointwise_ff.png\" width=\"320\" height=\"240\" align=\"right\">\n",
    "Each of the transformer blocks will have a \"scratch-pad\" memory which is applied to each position separately and identically, it's consists of two fully-connected layers with a ReLU activation in between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n",
    "While the linear transformations are the same across different positions, they use different parameters from layer to layer.\n",
    "\n",
    "The input/output shapes should remain the same [batch, seq_len, d_model]\n",
    "\n",
    "The LayerNorms, like batch norm serves to make trainign less dependent of hyper-parameters, and their residual connection helps vanishing gradient issues.\n",
    "<img src=\"./imgs/batch_vs_layer_norm.png\" width=\"320\" height=\"240\" align=\"down\">\n",
    "\n",
    "The transformer architecture uses the LayerNorm also because it's more common on NLP applications but because the BatchNorm gives more difficulties to parallelize accross multiple GPUs, the authors and other people who choses to use Transformers for other applications on ther than NLP, keep using LayerNorm.\n",
    "\n",
    "Another cool detail to pay attention is that due to the fact that the Attention will mix the sequnce and feature dimensions, the LayerNorm will actually work accross both Sequence and Feature dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_wise_ff = PositionWiseFeedForward(d_model=512, d_ff=2048)\n",
    "x = torch.rand(3, 60, 512)\n",
    "out = point_wise_ff(x)\n",
    "print(f'Point wise feed-forward shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "x = torch.rand(64, 43, 512)\n",
    "encoder_block_out = encoder(x)\n",
    "print(f'Encoder block output shape: {out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = DecoderBlock(d_model=512, num_heads=8, d_ff=2048)\n",
    "x = torch.rand(64, 50, 512)\n",
    "decoder_block_out, _, _ = encoder(x, encoder_output=encoder_block_out, look_ahead_mask=None, padding_mask=None)\n",
    "print(f'Decoder block output shape: {decoder_block_out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "This layer will encompass all the elements needed on the transformer encoder:\n",
    "* Embedding\n",
    "* Positional Encoder\n",
    "* Multi-Head Self-Attention\n",
    "* Point-Wise feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_encoder = TransformerEncoder(\n",
    "    n_x=2, d_model=512, num_heads=8, d_ff=2048, input_vocab_size=8500, max_len=10000)\n",
    "\n",
    "#print(sample_encoder)\n",
    "\n",
    "# Input shape: [batch, seq_len] the input will be a batch of sequences of word indexes (integers)\n",
    "input_sentence = torch.randint(0, 200, (12, 62))\n",
    "encoder_out = sample_encoder(x=input_sentence, mask=None)\n",
    "print(f'Encoder output shape: {encoder_out.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "This layer will encompass all the elements needed on the transformer decoder:\n",
    "* Embedding (Same as encoder)\n",
    "* Positional Encoder\n",
    "* Multi-Head Decoder-Encoder Attention\n",
    "* Multi-Head Masked Self Attention\n",
    "* Point-Wise feed Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_decoder = TransformerDecoder(\n",
    "    n_x=2, d_model=512, num_heads=8, d_ff=2048, output_vocab_size=8000, max_len=5000)\n",
    "\n",
    "#print(sample_decoder)\n",
    "\n",
    "# Input shape: [batch, seq_len] the input will be a batch of sequences of word indexes (integers)\n",
    "previous_generated_sentences = torch.randint(0, 200, (12, 26))\n",
    "decoder_out, attn_weights = sample_decoder(\n",
    "    x=previous_generated_sentences, \n",
    "    encoder_output=encoder_out, \n",
    "    look_ahead_mask=None, padding_mask=None)\n",
    "print(f'Decoder output shape: {decoder_out.shape}')\n",
    "print(f\"Decoder attention weights shape: {attn_weights['decoder_layer2_attn_weight_2'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "The masking trick will be used on transformers to ensure the following:\n",
    "* Avoid that the Decoder sees more information on the future that it allows to see\n",
    "* Avoid computing on PAD tokens (tokens with value zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = create_look_ahead_mask(size=20)\n",
    "print('Masks shape:', masks.shape)\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(masks);\n",
    "\n",
    "# What we can see at position 4\n",
    "masks[4].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid PAD (zero-token)\n",
    "array = np.array([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0]])\n",
    "array = torch.from_numpy(array)\n",
    "padding_mask = create_padding_mask(array)\n",
    "print(padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_ahead = create_look_ahead_mask(size=5)\n",
    "torch.maximum(padding_mask, look_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Model\n",
    "Now we mix all the Encoder/Decoder modules on the full architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trf = TransformerEncoderDecoder(n_x=2, d_model=512, \n",
    "                                        num_heads=8, d_ff=2048,\n",
    "                                        input_vocab_size=8500, \n",
    "                                        output_vocab_size=8000,\n",
    "                                        max_len_input=10000, \n",
    "                                        max_len_output=6000)\n",
    "\n",
    "input = torch.randint(0, 200, (64, 38))\n",
    "current_output = torch.randint(0, 200, (64, 36))\n",
    "\n",
    "# Expected shape: [batch_size, tar_seq_len, target_vocab_size]\n",
    "fn_out, _ = trf(inputs=input, current_output=current_output)\n",
    "print('fn_out.shape:', fn_out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
